{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification using Logistic Regression and K-Means\n",
    "\n",
    "This notebook demonstrates image classification on a facial expression dataset using:\n",
    "- **Logistic Regression** (supervised classifier)\n",
    "- **K-Means** (unsupervised clustering, adapted for classification)\n",
    "\n",
    "We will work with up to 5 emotion classes from the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing Required Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T00:51:28.213360Z",
     "start_time": "2025-12-03T00:51:27.981234Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import cv2\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading the Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T00:51:32.896634Z",
     "start_time": "2025-12-03T00:51:31.491678Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n",
      "Training data shape: (28709, 2)\n",
      "Columns: ['emotion', 'pixels']\n",
      "\n",
      "First few rows:\n",
      "   emotion                                             pixels\n",
      "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...\n",
      "1        0  151 150 147 155 148 133 111 140 170 174 182 15...\n",
      "2        2  231 212 156 164 174 138 161 173 182 200 106 38...\n",
      "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...\n",
      "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...\n",
      "\n",
      "Unique emotion classes: [np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6)]\n",
      "Number of classes: 7\n"
     ]
    }
   ],
   "source": [
    "# Load training data\n",
    "print(\"Loading training data...\")\n",
    "train_df = pd.read_csv('Data/train.csv')\n",
    "\n",
    "# Check the structure\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "print(f\"Columns: {train_df.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(train_df.head())\n",
    "\n",
    "# Check unique emotion classes\n",
    "unique_emotions = sorted(train_df['emotion'].unique())\n",
    "print(f\"\\nUnique emotion classes: {unique_emotions}\")\n",
    "print(f\"Number of classes: {len(unique_emotions)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Limiting to 5 Classes (if needed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit to maximum 5 classes\n",
    "max_classes = 5\n",
    "if len(unique_emotions) > max_classes:\n",
    "    print(f\"Limiting dataset to first {max_classes} classes...\")\n",
    "    train_df = train_df[train_df['emotion'].isin(unique_emotions[:max_classes])]\n",
    "    print(f\"New dataset shape: {train_df.shape}\")\n",
    "    unique_emotions = sorted(train_df['emotion'].unique())\n",
    "    print(f\"Classes used: {unique_emotions}\")\n",
    "else:\n",
    "    print(f\"Dataset already has {len(unique_emotions)} classes (‚â§ {max_classes})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Converting Pixel Strings to Images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixels_to_image(pixel_string, img_size=(48, 48)):\n",
    "    \"\"\"\n",
    "    Convert space-separated pixel string to numpy array image.\n",
    "    \n",
    "    Parameters:\n",
    "    - pixel_string: String of space-separated pixel values\n",
    "    - img_size: Tuple (height, width) for image dimensions\n",
    "    \n",
    "    Returns:\n",
    "    - Image as numpy array of shape (height, width)\n",
    "    \"\"\"\n",
    "    pixels = np.array(pixel_string.split(), dtype='float32')\n",
    "    image = pixels.reshape(img_size)\n",
    "    return image\n",
    "\n",
    "# Convert pixel strings to images\n",
    "print(\"Converting pixel strings to images...\")\n",
    "images = np.array([pixels_to_image(pixel_str) for pixel_str in train_df['pixels']])\n",
    "labels = train_df['emotion'].values\n",
    "\n",
    "print(f\"Images shape: {images.shape}\")\n",
    "print(f\"Labels shape: {labels.shape}\")\n",
    "print(f\"Pixel value range: [{images.min():.1f}, {images.max():.1f}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualizing Sample Images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample images from each class\n",
    "emotion_labels = {0: 'Angry', 1: 'Disgust', 2: 'Fear', 3: 'Happy', 4: 'Sad', 5: 'Surprise', 6: 'Neutral'}\n",
    "\n",
    "fig, axes = plt.subplots(1, len(unique_emotions), figsize=(15, 3))\n",
    "if len(unique_emotions) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for idx, emotion in enumerate(unique_emotions):\n",
    "    # Find first image of this emotion\n",
    "    emotion_idx = np.where(labels == emotion)[0][0]\n",
    "    axes[idx].imshow(images[emotion_idx], cmap='gray')\n",
    "    axes[idx].set_title(f\"Class {emotion}\\n{emotion_labels.get(emotion, 'Unknown')}\")\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.suptitle('Sample Images from Each Class', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Image Preprocessing\n",
    "\n",
    "We'll apply several preprocessing techniques from the tutorial:\n",
    "- Normalization\n",
    "- Grayscale conversion (already grayscale)\n",
    "- Optional: Histogram equalization\n",
    "- Optional: Gaussian blur for noise reduction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images(images, apply_hist_eq=False, apply_gaussian=False):\n",
    "    \"\"\"\n",
    "    Preprocess images: normalize and optionally apply histogram equalization and Gaussian blur.\n",
    "    \n",
    "    Parameters:\n",
    "    - images: Array of images (n_samples, height, width)\n",
    "    - apply_hist_eq: Whether to apply histogram equalization\n",
    "    - apply_gaussian: Whether to apply Gaussian blur\n",
    "    \n",
    "    Returns:\n",
    "    - Preprocessed images\n",
    "    \"\"\"\n",
    "    processed_images = images.copy().astype('float32')\n",
    "    \n",
    "    # Normalize to [0, 1]\n",
    "    for i in range(len(processed_images)):\n",
    "        img = processed_images[i]\n",
    "        img_min, img_max = img.min(), img.max()\n",
    "        if img_max > img_min:\n",
    "            processed_images[i] = (img - img_min) / (img_max - img_min)\n",
    "    \n",
    "    # Convert to uint8 for OpenCV operations\n",
    "    processed_images_uint8 = (processed_images * 255).astype('uint8')\n",
    "    \n",
    "    # Apply histogram equalization if requested\n",
    "    if apply_hist_eq:\n",
    "        for i in range(len(processed_images_uint8)):\n",
    "            processed_images_uint8[i] = cv2.equalizeHist(processed_images_uint8[i])\n",
    "    \n",
    "    # Apply Gaussian blur if requested\n",
    "    if apply_gaussian:\n",
    "        for i in range(len(processed_images_uint8)):\n",
    "            processed_images_uint8[i] = cv2.GaussianBlur(processed_images_uint8[i], (3, 3), 0)\n",
    "    \n",
    "    # Convert back to [0, 1] range\n",
    "    processed_images = processed_images_uint8.astype('float32') / 255.0\n",
    "    \n",
    "    return processed_images\n",
    "\n",
    "# Apply preprocessing\n",
    "print(\"Preprocessing images...\")\n",
    "processed_images = preprocess_images(images, apply_hist_eq=False, apply_gaussian=False)\n",
    "print(f\"Preprocessed images shape: {processed_images.shape}\")\n",
    "print(f\"Preprocessed pixel value range: [{processed_images.min():.3f}, {processed_images.max():.3f}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Flattening Images for Classification\n",
    "\n",
    "Both Logistic Regression and K-Means require 2D input (samples √ó features).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten images to feature vectors\n",
    "n_samples, height, width = processed_images.shape\n",
    "X = processed_images.reshape(n_samples, -1)  # Flatten to (n_samples, height*width)\n",
    "y = labels\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Labels shape: {y.shape}\")\n",
    "print(f\"Number of features per image: {X.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train-Test Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"\\nClass distribution in training set:\")\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "for emotion, count in zip(unique, counts):\n",
    "    print(f\"  Class {emotion}: {count} samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Feature Scaling\n",
    "\n",
    "Standardizing features is important for Logistic Regression and can help K-Means converge faster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Scaled training features - Mean: {X_train_scaled.mean():.4f}, Std: {X_train_scaled.std():.4f}\")\n",
    "print(f\"Scaled test features - Mean: {X_test_scaled.mean():.4f}, Std: {X_test_scaled.std():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Logistic Regression Classifier\n",
    "in image data there is 3 CSV I WANT TO CREATE. Logistic regression & kmeans as classifiers on an image dataset (5 classes at maximum). IN JUPYTER NOTEBOOK \n",
    "\n",
    "HERE IS THE IMAGE PREPROCCESING TURTORIALS TA GAVE \n",
    "\n",
    "##%%\n",
    "#importing required Libraries\n",
    "import numpy as np\n",
    "import tensorflow\n",
    "import keras\n",
    "import os\n",
    "import glob\n",
    "from skimage import io\n",
    "import skimage\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "##%% md\n",
    "# Image Preprocessing in Python (Lecture Notebook)\n",
    "\n",
    "This notebook introduces fundamental image preprocessing techniques using **Python**, **OpenCV**, **scikit-image**, and **Keras**.\n",
    "\n",
    "We will cover:\n",
    "\n",
    "- Loading and visualizing images\n",
    "- RGB channels and grayscale conversion\n",
    "- Normalization\n",
    "- Data augmentation (flipping and rotation)\n",
    "- Contrast enhancement (Histogram Equalization & CLAHE)\n",
    "- Smoothing filters (mean, Gaussian, median)\n",
    "- Edge detection (Sobel, Laplacian)\n",
    "- Sharpening with custom kernels\n",
    "- Thresholding and basic image segmentation\n",
    "\n",
    "##%% md\n",
    "## 1. Loading and Visualizing the Original Image\n",
    "\n",
    "In this step we:\n",
    "\n",
    "1. Load an image from disk using `skimage.io.imread`.\n",
    "2. Visualize it using `matplotlib`.\n",
    "\n",
    "Why this step is important:\n",
    "\n",
    "- It lets us inspect the **raw image**.\n",
    "- We can observe:\n",
    "- Resolution (width √ó height)\n",
    "- Colors and lighting\n",
    "- Any visible noise or blur\n",
    "- This serves as a **baseline** before applying preprocessing.\n",
    "\n",
    "> Make sure `fruits.png` is in the same folder as your notebook, or provide the full path.\n",
    "\n",
    "##%%\n",
    "import os\n",
    "# accessing an image file from the dataset classes\n",
    "image = io.imread('fruits.png')\n",
    "\n",
    "# plotting the original image\n",
    "i, (im1) = plt.subplots(1)\n",
    "i.set_figwidth(15)\n",
    "im1.imshow(image)\n",
    "##%% md\n",
    "## 2. Visualizing the Original Image and Its RGB Color Channels\n",
    "\n",
    "Color images are stored as 3D arrays: **(height, width, channels)**.\n",
    "\n",
    "- Channel 0 ‚Üí Red\n",
    "- Channel 1 ‚Üí Green\n",
    "- Channel 2 ‚Üí Blue\n",
    "\n",
    "In this section we:\n",
    "\n",
    "- Show the original image\n",
    "- Show each channel separately (R, G, B)\n",
    "\n",
    "Why this helps:\n",
    "\n",
    "- Understand which colors dominate the image.\n",
    "- See how information is distributed across channels.\n",
    "- Useful before performing color-based processing (e.g. segmentation, enhancement).\n",
    "\n",
    "##%%\n",
    "# plotting the original image and the RGB channels\n",
    "\n",
    "i, (im1, im2, im3, im4) = plt.subplots(1, 4, sharey=True) # sharey -> All subplots will share the same Y-axis.\n",
    "i.set_figwidth(20)\n",
    "print(image.shape)\n",
    "im1.imshow(image) #Original image\n",
    "im2.imshow(image[:, : , 0]) #Red\n",
    "im3.imshow(image[:, : , 1]) #Green\n",
    "im4.imshow(image[:, : , 2]) #Blue\n",
    "i.suptitle('Original & RGB image channels')\n",
    "##%% md\n",
    "The RGB image is converted into a **grayscale** representation using `skimage.color.rgb2gray()`.\n",
    "A grayscale image reduces the three color channels (Red, Green, and Blue) into a single intensity channel by applying a weighted sum that reflects human visual perception. This simplification is commonly used in many preprocessing tasks, such as edge detection, thresholding, and filtering, where color information is not required.\n",
    "\n",
    "The resulting grayscale image is then displayed using matplotlib with the 'gray' colormap to ensure proper visualization of intensity variations.\n",
    "##%% md\n",
    "## 3. Converting RGB Image to Grayscale\n",
    "\n",
    "We convert the RGB image into a **single-channel grayscale** image using:\n",
    "\n",
    "```python\n",
    "skimage.color.rgb2gray(image) ---> it is automatically normalizes the grayscale image to the range [0, 1].\n",
    "\n",
    "##%%\n",
    "gray_image = skimage.color.rgb2gray(image)\n",
    "plt.imshow(gray_image, cmap = 'gray')\n",
    "##%% md\n",
    "## 4. Normalization (Min‚ÄìMax Scaling)\n",
    "\n",
    "We normalize the grayscale image to the range **[0, 1]** using:\n",
    "\n",
    "$$\n",
    "\\text{norm}(I) = \\frac{I - I_{\\min}}{I_{\\max} - I_{\\min}}\n",
    "$$\n",
    "\n",
    "Why normalize?\n",
    "\n",
    "- Standardizes pixel values.\n",
    "- Improves numerical stability for ML/DL models.\n",
    "- Helps networks train faster and more reliably.\n",
    "- Makes images from different sources more comparable.\n",
    "\n",
    "We will compute the normalized image and display it.\n",
    "\n",
    "##%%\n",
    "norm_image = (gray_image - np.min(gray_image)) / (np.max(gray_image) - np.min(gray_image))\n",
    "plt.imshow(norm_image)\n",
    "##%% md\n",
    "## 5. Installing `keras_preprocessing` (For Data Augmentation)\n",
    "\n",
    "To perform geometric transformations (data augmentation) in Keras, we use:\n",
    "\n",
    "- `ImageDataGenerator` from `keras_preprocessing.image`\n",
    "\n",
    "If the package is not already installed, we install it using `pip` inside the notebook.\n",
    "\n",
    "##%%\n",
    "#!pip install keras_preprocessing\n",
    "##%% md\n",
    "## 6. Preparing Image Batch for Data Augmentation\n",
    "\n",
    "`ImageDataGenerator` expects input as a batch of images with shape:\n",
    "\n",
    "- `(batch_size, height, width, channels)`\n",
    "\n",
    "We will:\n",
    "\n",
    "1. Convert our image to `float32`.\n",
    "2. Add a new axis to create a batch of size 1 using `np.expand_dims`.\n",
    "\n",
    "This will be used for flipping and rotation examples.\n",
    "\n",
    "##%%\n",
    "from numpy import expand_dims\n",
    "print(image.shape)\n",
    "# Ensure we use the RGB image for augmentation\n",
    "samples = expand_dims(image.astype('float32'), axis=0) # shape: (1, H, W, 3)\n",
    "\n",
    "\n",
    "##%%\n",
    "print(\"Batch shape:\", samples.shape)\n",
    "\n",
    "##%% md\n",
    "# Geometric Transformations\n",
    "In this step, we apply **geometric transformations** to the images using the `ImageDataGenerator` class from `Keras`. Geometric transformations are a form of data augmentation.\n",
    "\n",
    "We use `ImageDataGenerator` to apply:\n",
    "\n",
    "- `horizontal_flip=True` ‚Üí flip left‚Äìright\n",
    "- `vertical_flip=True` ‚Üí flip top‚Äìbottom\n",
    "\n",
    "Why do this?\n",
    "\n",
    "- Data augmentation: increases dataset size without collecting new images.\n",
    "- Makes models more robust to orientation changes.\n",
    "- Helps prevent overfitting.\n",
    "\n",
    "Explain:\n",
    "\n",
    "* Creating an ImageDataGenerator instance with the desired transformations.\n",
    "\n",
    "* Generating batches of images using the `.flow()` method, which produces transformed images on the fly.\n",
    "\n",
    "* Iterating through the generated images and converting them to unsigned integers (`uint8`) for proper visualization.\n",
    "\n",
    "* Plotting the transformed images side by side to observe the effects of horizontal and vertical flips.\n",
    "\n",
    "We will:\n",
    "\n",
    "1. Create an `ImageDataGenerator` with flipping options.\n",
    "2. Generate 3 augmented images.\n",
    "3. Display them side by side.\n",
    "##%%\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# ImageDataGenerator for flipping\n",
    "datagen = ImageDataGenerator(horizontal_flip=True, vertical_flip=True)\n",
    "\n",
    "# Create an iterator\n",
    "it = datagen.flow(samples, batch_size=1) #batch_size=1 ->Each time you call the iterator, it returns only 1 augmented image.\n",
    "\n",
    "# Plot some flipped images\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))\n",
    "\n",
    "for i in range(3):\n",
    "# Get a batch (1 image), take first image\n",
    "batch = next(it)[0].astype('uint8')\n",
    "axes[i].imshow(batch)\n",
    "axes[i].set_title(f\"Flip Sample {i+1}\")\n",
    "axes[i].axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"Horizontal & Vertical Flips\")\n",
    "plt.show()\n",
    "\n",
    "##%% md\n",
    "## 8. Geometric Transformations ‚Äì Rotation\n",
    "\n",
    "\n",
    "Apply `random rotation transformations` to the images as part of data augmentation. The `rotation_range` parameter specifies the maximum rotation angle (in degrees) for randomly rotating images. Here, `rotation_range=40` allows images to rotate within ¬±40 degrees.\n",
    "\n",
    "**Why use rotation?**\n",
    "- Simulates different orientations.\n",
    "- Helps models generalize better when objects are rotated in real-world data.\n",
    "##%%\n",
    "# ImageDataGenerator for rotation\n",
    "# Fills empty pixels with the value of the nearest pixel\n",
    "# The image will be randomly rotated between ‚Äì40¬∞ and +40¬∞.\n",
    "datagen = ImageDataGenerator(rotation_range=40, fill_mode='nearest') #\n",
    "\n",
    "# Create an iterator\n",
    "it = datagen.flow(samples, batch_size=1)\n",
    "\n",
    "# Plot some flipped images\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))\n",
    "\n",
    "for i in range(3):\n",
    "# Get a batch (1 image), take first image\n",
    "batch = next(it)[0].astype('uint8')\n",
    "axes[i].imshow(batch)\n",
    "axes[i].set_title(f\"Rotation Sample {i+1}\")\n",
    "axes[i].axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"Random Rotations (¬±40¬∞)\")\n",
    "plt.show()\n",
    "##%% md\n",
    "## 9. Histogram Equalization for Contrast Enhancement (ÿ≤ŸäÿßÿØÿ© ÿßŸÑÿ™ÿ®ÿßŸäŸÜ + ÿ™ÿ≠ÿ≥ŸäŸÜ ÿßŸÑŸàÿ∂Ÿàÿ≠)\n",
    "\n",
    "We now switch to **OpenCV** (`cv2`) for some operations.\n",
    "\n",
    "### Goal:\n",
    "- Improve image contrast using **histogram equalization**.\n",
    "\n",
    "Steps:\n",
    "1. Load the image in **grayscale**.\n",
    "2. Apply `cv2.equalizeHist`.\n",
    "3. Compare original vs equalized images.\n",
    "\n",
    "Histogram equalization:\n",
    "- Spreads pixel intensities (ÿ®ÿ™Ÿàÿ≤Ÿëÿπ ŸÇŸäŸÖ ÿßŸÑÿ•ÿ∂ÿßÿ°ÿ©) across the full range `[0, 255]`.\n",
    "- Makes dark regions brighter and bright regions clearer. (ÿ•ÿ∏Ÿáÿßÿ± ÿßŸÑÿ™ŸÅÿßÿµŸäŸÑ ÿßŸÑŸÖÿÆŸÅŸäÿ©)\n",
    "- Helpful when the image looks too dark or too washed out. (ŸÖŸÅŸäÿØ ŸÅŸä ÿßŸÑÿµŸàÿ± ÿ∞ÿßÿ™ ÿßŸÑÿ•ÿ∂ÿßÿ°ÿ© ÿßŸÑÿ≥Ÿäÿ¶ÿ©)\n",
    "\n",
    "##%%\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load grayscale image\n",
    "img = cv2.imread(\"fruits.png\", 0) # 0 ‚Üí grayscale\n",
    "\n",
    "# Histogram equalization\n",
    "equalized = cv2.equalizeHist(img)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Original Grayscale Image\")\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Equalized Image\")\n",
    "plt.imshow(equalized, cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "##%% md\n",
    "## 10. Visualizing Histograms Before and After Equalization\n",
    "\n",
    "To better understand the effect of **histogram equalization**, we plot the pixel intensity histograms of the original and equalized images.\n",
    "\n",
    "* `The original histogram` shows the distribution of pixel values in the grayscale image. In many cases, the values are concentrated in a narrow range, which can make the image appear dark or washed out.\n",
    "\n",
    "* `The equalized histogram` illustrates how the pixel intensities have been redistributed across the full range (0‚Äì255). This spreading of values increases the contrast and highlights previously hidden details.\n",
    "\n",
    "By comparing these histograms side by side, it becomes evident that histogram equalization effectively enhances image contrast while preserving the overall structure of the image. Visual inspection alongside histogram analysis provides a clear and quantitative understanding of the preprocessing step‚Äôs impact.\n",
    "##%%\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Original Histogram\")\n",
    "plt.hist(img.ravel(), 256, [0, 256])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Equalized Histogram\")\n",
    "plt.hist(equalized.ravel(), 256, [0, 256])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "##%% md\n",
    "## 11. Contrast Enhancement Using CLAHE\n",
    "\n",
    "**CLAHE** = Contrast Limited Adaptive Histogram Equalization.\n",
    "- used to improve the contrast of a color image\n",
    "\n",
    "Differences vs normal histogram equalization:\n",
    "\n",
    "- Works on **small tiles** instead of whole image. \n",
    "- Limits contrast amplification (avoids over-enhancing noise). ( Ÿäÿ≠ÿØ ŸÖŸÜ ÿ™ÿ∂ÿÆŸäŸÖ ÿßŸÑÿ™ÿ®ÿßŸäŸÜ (Ÿäÿ™ÿ¨ŸÜÿ® ÿßŸÑÿ™ÿ¥ŸàŸäÿ¥ ÿßŸÑŸÖŸÅÿ±ÿ∑).)\n",
    "- Very useful for images with **uneven lighting**. (ÿßŸÑÿ•ÿ∂ÿßÿ°ÿ© ÿ∫Ÿäÿ± ÿßŸÑŸÖÿ™ÿ≥ÿßŸàŸäÿ©)\n",
    "- A small amount of noise may appear due to processing each tile separately.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Convert BGR image to LAB color space. (LAB separates lightness (L) from color (A and B).)\n",
    "2. Apply CLAHE on the L (lightness) channel.\n",
    "3. Merge channels and convert back to BGR/RGB.\n",
    "4. Compare before and after.\n",
    "\n",
    "**LAB channels:**\n",
    "- L = Lightness (0‚Äì255)\n",
    "- A = Green‚ÄìRed scale\n",
    "- B = Blue‚ÄìYellow scale\n",
    "##%% md\n",
    "**clipLimit=3.0**\n",
    "- Prevents over-amplifying noise\n",
    "- Higher value ‚Üí stronger contrast\n",
    "\n",
    "**tileGridSize=(8, 8)**\n",
    "- Image is divided into 8√ó8 regions\n",
    "- Contrast enhancement is applied locally\n",
    "- Good for images with uneven lighting\n",
    "##%%\n",
    "# Load image (BGR)\n",
    "img_color = cv2.imread(\"fruits.png\")\n",
    "\n",
    "# Convert to LAB color space\n",
    "lab = cv2.cvtColor(img_color, cv2.COLOR_BGR2LAB)\n",
    "l, a, b = cv2.split(lab)\n",
    "\n",
    "# Apply CLAHE on L-channel (lightness)\n",
    "clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))\n",
    "cl = clahe.apply(l)\n",
    "\n",
    "# Merge channels and convert back to BGR\n",
    "lab_clahe = cv2.merge((cl, a, b))\n",
    "final = cv2.cvtColor(lab_clahe, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "# Show Results\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Original Image\")\n",
    "plt.imshow(cv2.cvtColor(img_color, cv2.COLOR_BGR2RGB))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"After CLAHE\")\n",
    "plt.imshow(cv2.cvtColor(final, cv2.COLOR_BGR2RGB))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "##%% md\n",
    "## 12. Low-Pass Filters (Smoothing / Blurring)\n",
    "\n",
    "In this step, we apply a **Mean Filter** to the grayscale image. The mean filter is a type of low-pass filter that **smooths** the image by reducing intensity variations between neighboring pixels. **It works by replacing each pixel value with the average of its surrounding pixels defined by a kernel** (in this case, a 3√ó3 window).\n",
    "\n",
    "\n",
    "We will apply:\n",
    "\n",
    "1. **Mean Filter** (Ÿäÿ£ÿÆÿ∞ ŸÖÿ™Ÿàÿ≥ÿ∑ ŸÇŸäŸÖ ÿßŸÑÿ®ŸÉÿ≥ŸÑÿßÿ™ ÿØÿßÿÆŸÑ ŸÜÿßŸÅÿ∞ÿ© (Kernel).)\n",
    "2. **Gaussian Blur**\n",
    "3. **Median Filter** \n",
    "\n",
    "Why smoothing?\n",
    "\n",
    "- Reduces noise.\n",
    "- Softens edges.\n",
    "- Often used before edge detection or segmentation.\n",
    "##%%\n",
    "# Reload grayscale image for filtering\n",
    "img = cv2.imread(\"fruits.png\", 0)\n",
    "\n",
    "# Mean (average) filter with 3x3 kernel\n",
    "mean = cv2.blur(img, (3, 3))\n",
    "\n",
    "plt.imshow(mean, cmap=\"gray\")\n",
    "plt.title(\"Mean Filter (3x3)\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "##%%\n",
    "# Gaussian blur with 5x5 kernel, sigma = 0 (auto)\n",
    "gaussian = cv2.GaussianBlur(img, (5, 5), 0)\n",
    "\n",
    "plt.imshow(gaussian, cmap=\"gray\")\n",
    "plt.title(\"Gaussian Blur (5x5)\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "##%%\n",
    "median = cv2.medianBlur(img, 5) # 5x5 neighborhood\n",
    "\n",
    "plt.imshow(median, cmap=\"gray\")\n",
    "plt.title(\"Median Filter (5x5)\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "##%% md\n",
    "## 13. High-Pass Filters (Edge Detection & Sharpening)\n",
    "\n",
    "Edge Detection Using **Sobel Filter**\n",
    "\n",
    "The Sobel operator computes the gradient of pixel intensities(ÿ™ÿØÿ±ÿ¨ ÿ¥ÿØÿ© ÿßŸÑÿ®ŸÉÿ≥ŸÑ) in both horizontal (X) and vertical (Y) directions, highlighting regions with significant intensity changes.\n",
    "\n",
    "* sobel(x) detects vertical edges by calculating horizontal intensity gradients.\n",
    "\n",
    "* sobel(y) detects horizontal edges by calculating vertical intensity gradients.\n",
    "\n",
    "Edge detection is a fundamental preprocessing step in computer vision, used to identify object boundaries, enhance features for segmentation, and extract structural information from images. Visualizing both X and Y gradients separately allows us to analyze the directionality and strength of edges in the image.\n",
    "##%% md\n",
    "**Understanding dx and dy**\n",
    "\n",
    "**- dx = 1, dy = 0 ‚Üí detect changes along X-axis**\n",
    "- Finds vertical edges\n",
    "- (Because vertical edges change in X direction)\n",
    "\n",
    "**- dx = 0, dy = 1 ‚Üí detect changes along Y-axis**\n",
    "- Finds horizontal edges\n",
    "- (Because horizontal edges change in Y direction)\n",
    "##%%\n",
    "sobelx = cv2.Sobel(img, cv2.CV_64F, 1, 0, ksize=3) # depth,1->SobelX, 0->SobelY\n",
    "sobely = cv2.Sobel(img, cv2.CV_64F, 0, 1, ksize=3) \n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(sobelx, cmap=\"gray\")\n",
    "plt.title(\"Sobel X (Vertical Edges)\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(sobely, cmap=\"gray\")\n",
    "plt.title(\"Sobel Y (Horizontal Edges)\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "##%% md\n",
    "### üîç Laplacian Filter (Simplified Explanation)\n",
    "\n",
    "The **Laplacian filter** is a high-pass filter used to detect edges in an image.\n",
    "\n",
    "- It responds to **sharp changes in intensity**.\n",
    "- Unlike Sobel (which finds horizontal or vertical edges separately), \n",
    "**Laplacian detects edges in all directions at once**.\n",
    "- This makes it useful for highlighting **fine details** and **object boundaries**.\n",
    "\n",
    "The resulting image shows bright edges and suppresses smooth, low-detail areas.\n",
    "\n",
    "##%%\n",
    "laplacian = cv2.Laplacian(img, cv2.CV_64F)\n",
    "\n",
    "plt.imshow(laplacian, cmap=\"gray\")\n",
    "plt.title(\"Laplacian (All Edges)\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "##%% md\n",
    "### Sharpening Filter (Custom Kernel)\n",
    "\n",
    "In this step, we apply a sharpening filter to enhance the details and edges of the grayscale image. The filter is implemented using a convolution kernel that emphasizes the central pixel relative to its neighbors:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "0 & -1 & 0 \\\\\n",
    "-1 & 5 & -1 \\\\\n",
    "0 & -1 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "- The center pixel is multiplied by 5.\n",
    "- Neighboring pixels are subtracted.\n",
    "- This increases contrast at edges ‚Üí image looks sharper.\n",
    "- This is a **balanced** sharpening kernel.\n",
    "\n",
    "This kernel increases the contrast between a pixel and its surrounding pixels, effectively highlighting edges and fine structures while maintaining the overall brightness of the image. Sharpening is a common preprocessing technique to improve the visibility of important features, which can be useful in tasks such as object recognition, segmentation, and feature extraction.\n",
    "##%%\n",
    "kernel = np.array([[0,-1,0],\n",
    "[-1,5,-1],\n",
    "[0,-1,0]])\n",
    "\n",
    "sharpened = cv2.filter2D(img, -1, kernel)\n",
    "\n",
    "plt.imshow(sharpened, cmap=\"gray\")\n",
    "plt.title(\"Sharpened Image\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "##%% md\n",
    "## 14. Segmentation with Thresholding\n",
    "\n",
    "### üåì Binary Thresholding\n",
    "\n",
    "In this step, we apply binary thresholding to the grayscale image. Thresholding is a fundamental technique in image segmentation.\n",
    "\n",
    "Binary thresholding separates the image into **two groups of pixels**: \n",
    "- **Foreground (white)** \n",
    "- **Background (black)** \n",
    "\n",
    "We use `cv2.threshold()` with a threshold value of **127**:\n",
    "\n",
    "- Pixels with intensity values **‚â• 127** ‚Üí become **255 (white)** \n",
    "- Pixels with intensity values **< 127** ‚Üí become **0 (black)** \n",
    "\n",
    "This creates a clean **black-and-white** image that highlights the main shapes and removes most background noise. \n",
    "Binary thresholding is commonly used before tasks like object detection, shape analysis, and feature extraction.\n",
    "\n",
    "##%%\n",
    "# Global binary thresholding\n",
    "_, thresh = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY) #255 maxval->The value assigned to pixels above the threshold\n",
    "\n",
    "plt.imshow(thresh, cmap=\"gray\")\n",
    "plt.title(\"Binary Threshold (T = 127)\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "##%% md\n",
    "### üåì Adaptive Thresholding (Simplified)\n",
    "\n",
    "Adaptive thresholding is used when the image has **uneven lighting**. \n",
    "Instead of using one global threshold, it calculates a **separate threshold for each small region** of the image.\n",
    "\n",
    "We use `cv2.adaptiveThreshold()` with these settings:\n",
    "\n",
    "- **ADAPTIVE_THRESH_MEAN_C** \n",
    "Threshold = (mean of local neighborhood) ‚àí C\n",
    "\n",
    "- **THRESH_BINARY** \n",
    "Output pixels become either **0 (black)** or **255 (white)**.\n",
    "\n",
    "- **blockSize = 11** \n",
    "Size of the small region used to compute the local mean.\n",
    "\n",
    "- **C = 2** \n",
    "A small constant that adjusts the threshold.\n",
    "\n",
    "This method produces a cleaner binary image in areas with different lighting, making segmentation more accurate.\n",
    "\n",
    "##%%\n",
    "adaptive = cv2.adaptiveThreshold(img, 255,\n",
    "cv2.ADAPTIVE_THRESH_MEAN_C,\n",
    "cv2.THRESH_BINARY,\n",
    "11, 2)\n",
    "\n",
    "plt.imshow(adaptive, cmap=\"gray\")\n",
    "plt.title(\"Adaptive Threshold\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "##%% md\n",
    "### üìà Otsu‚Äôs Thresholding (Automatic Binarization)\n",
    "\n",
    "Otsu‚Äôs method is a global thresholding technique that **automatically chooses the best threshold value** based on the image histogram. \n",
    "No need to set a threshold manually.\n",
    "\n",
    "Using `cv2.threshold()` with the `THRESH_OTSU` flag:\n",
    "\n",
    "- The optimal threshold is computed from the histogram.\n",
    "- Pixels **above** that value ‚Üí **255 (white)**\n",
    "- Pixels **below** ‚Üí **0 (black)**\n",
    "\n",
    "Otsu‚Äôs method works best when the image has a **bimodal histogram** (two clear intensity groups). It produces a clean binary image even when the contrast varies.\n",
    "\n",
    "##%%\n",
    "# Otsu's thresholding (automatic global threshold)\n",
    "_, otsu = cv2.threshold(\n",
    "img,\n",
    "0, # ignored when using OTSU\n",
    "255,\n",
    "cv2.THRESH_BINARY + cv2.THRESH_OTSU\n",
    ")\n",
    "\n",
    "plt.imshow(otsu, cmap=\"gray\")\n",
    "plt.title(\"Otsu Thresholding\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "##%% md\n",
    "## 15. Summary of Techniques Covered\n",
    "\n",
    "In this notebook, you learned how to:\n",
    "\n",
    "- Load and display images with `skimage` and `matplotlib`\n",
    "- Visualize RGB channels and convert to grayscale\n",
    "- Normalize pixel intensities to [0, 1]\n",
    "- Use `ImageDataGenerator` for:\n",
    "- Flipping\n",
    "- Rotation\n",
    "- Enhance contrast using:\n",
    "- Histogram Equalization\n",
    "- CLAHE (local contrast)\n",
    "- Apply smoothing filters:\n",
    "- Mean, Gaussian, Median\n",
    "- Detect edges using:\n",
    "- Sobel and Laplacian filters\n",
    "- Sharpen images using a custom convolution kernel\n",
    "- Perform basic segmentation with:\n",
    "- Global thresholding\n",
    "- Adaptive thresholding\n",
    "- Otsu's method\n",
    "\n",
    "These operations form a strong foundation for **image preprocessing** in computer vision pipelines before feeding images into machine learning or deep learning models.\n",
    "\n",
    "\n",
    "\n",
    "AND CLASSIFIER USING KNN HE GAVE TOO \n",
    "\n",
    "\n",
    "##%% md\n",
    "# K-Nearest Neighbors (KNN) for Image Classification\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook demonstrates the application of the **K-Nearest Neighbors (KNN)** algorithm to the task of **image classification**.\n",
    "\n",
    "KNN is a supervised learning method that classifies data points based on the labels of their nearest neighbors in the feature space. In the context of image classification, each image is represented as a vector of features, which may be raw pixel values or extracted features from a preprocessing pipeline.\n",
    "\n",
    "The primary objective of this notebook is to illustrate the process of:\n",
    "\n",
    "- Training a KNN classifier on a labeled image dataset.\n",
    "- Predicting the class labels of unseen test images.\n",
    "- Evaluating the model's performance using standard metrics.\n",
    "\n",
    "Throughout the notebook, we will:\n",
    "\n",
    "- Train a KNN classifier on the MNIST handwritten digits dataset.\n",
    "- Predict class labels for test images.\n",
    "- Evaluate performance using:\n",
    "- Accuracy\n",
    "- Confusion Matrix\n",
    "- Class-specific metrics (Precision, Recall, F1-score)\n",
    "\n",
    "##%%\n",
    "# Importing the dataset from Keras\n",
    "from keras.datasets import mnist\n",
    "\n",
    "# Load data: (x_train, y_train) for training, (x_test, y_test) for testing\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "##%% md\n",
    "## Loading the MNIST Dataset\n",
    "\n",
    "In this step, we load the **MNIST dataset** using `keras.datasets`.\n",
    "\n",
    "- `x_train`, `x_test`: contain the image data as NumPy arrays.\n",
    "- `y_train`, `y_test`: contain the corresponding labels (digits from 0 to 9).\n",
    "\n",
    "The dataset consists of:\n",
    "\n",
    "- 60,000 training images\n",
    "- 10,000 test images\n",
    "\n",
    "Each image is a grayscale `28 √ó 28` pixel image of a handwritten digit. \n",
    "- A grayscale image has one channel (not RGB).\n",
    "- Each pixel is a value from 0 ‚Üí 255:\n",
    "- 0 = black\n",
    "- 255 = white\n",
    "- values in between (1‚Äì254) = different shades of gray\n",
    "##%%\n",
    "# Checking the data types\n",
    "print(type(x_train))\n",
    "print(type(x_test))\n",
    "print(type(y_train))\n",
    "print(type(y_test))\n",
    "\n",
    "# Checking the shapes\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"x_test shape:\", x_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "##%%\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.gray() # Display images in black & white\n",
    "\n",
    "plt.figure(figsize=(10, 9)) # Adjust figure size\n",
    "\n",
    "# Display a grid of 3√ó3 images\n",
    "for i in range(9):\n",
    "plt.subplot(3, 3, i + 1)\n",
    "plt.imshow(x_train[i])\n",
    "\n",
    "plt.suptitle(\"Sample MNIST Training Images\")\n",
    "plt.show()\n",
    "\n",
    "##%%\n",
    "# Display the first 5 labels (true digit classes)\n",
    "for i in range(5):\n",
    "print(f\"Image {i} label:\", y_train[i])\n",
    "\n",
    "##%%\n",
    "# Checking the minimum and maximum values of x_train before normalization\n",
    "print(\"Before normalization:\")\n",
    "print(\"x_train min:\", x_train.min())\n",
    "print(\"x_train max:\", x_train.max())\n",
    "\n",
    "##%% md\n",
    "## Normalization for KNN\n",
    "\n",
    "We perform two preprocessing steps:\n",
    "\n",
    "1. **Data type conversion** \n",
    "- Convert from integer (`uint8`) to `float32` to support arithmetic operations and ML algorithms.\n",
    "\n",
    "2. **Normalization to [0, 1]** \n",
    "- Divide pixel values by 255.0.\n",
    "- Ensures that all features (pixels) contribute proportionally when computing distances.\n",
    "\n",
    "This is especially important for **KNN**, which relies on distance calculations in feature space. \n",
    "Without normalization, dimensions with larger numeric ranges could dominate the distance.\n",
    "\n",
    "##%%\n",
    "# Data Normalization\n",
    "\n",
    "# 1. Convert to float32\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# 2. Scale pixel values to [0, 1]\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "##%%\n",
    "# Checking the minimum and maximum values after normalization\n",
    "print(\"After normalization:\")\n",
    "print(\"x_train min:\", x_train.min())\n",
    "print(\"x_train max:\", x_train.max())\n",
    "\n",
    "##%% md\n",
    "## Flattening Images into Feature Vectors\n",
    "\n",
    "KNN (and many classical ML algorithms) expect input data as **2D arrays** of shape:\n",
    "\n",
    "- `(num_samples, num_features)`\n",
    "\n",
    "Originally:\n",
    "\n",
    "- `x_train` shape: `(num_samples, 28, 28)`\n",
    "\n",
    "After reshaping:\n",
    "\n",
    "- `X_train` shape: `(num_samples, 784)` where `784 = 28 √ó 28`.\n",
    "\n",
    "Each image becomes a **784-dimensional vector**, and KNN computes distances between these vectors to determine similarity.\n",
    "\n",
    "##%%\n",
    "# Reshaping input data\n",
    "# Each 28√ó28 image becomes a vector of length 784\n",
    "X_train = x_train.reshape(len(x_train), -1)\n",
    "X_test = x_test.reshape(len(x_test), -1)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "\n",
    "##%% md\n",
    "## K-Nearest Neighbors (KNN) as a Lazy Learner\n",
    "\n",
    "K-Nearest Neighbors (KNN) is considered a **lazy learning algorithm**.\n",
    "\n",
    "- Unlike **eager learning** methods (e.g., decision trees, neural networks) that **learn a global model** during training,\n",
    "- A **lazy learner** defers most computation until **prediction time**.\n",
    "- Lazy learners do NOT build a model during training.\n",
    "- They simply store the data and wait until a prediction is needed.\n",
    "\n",
    "### Key characteristics\n",
    "\n",
    "1. **Training Phase**\n",
    "- KNN does not learn explicit model parameters.\n",
    "- The `fit()` method simply **stores the training data and labels**.\n",
    "\n",
    "2. **Prediction Phase**\n",
    "- For each new input:\n",
    "- Computes the distance to all stored training points.\n",
    "- Finds the `k` nearest neighbors.\n",
    "- Predicts the class using **majority voting** among these neighbors.\n",
    "\n",
    "In this notebook, we will use `k = 3`, meaning each test image will be classified based on its 3 closest training images.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "When you choose k=3, the model:\n",
    "- Finds the 3 closest training samples to the test point\n",
    "- Looks at their labels\n",
    "- The majority vote becomes the prediction\n",
    "##%%\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Choose number of neighbors\n",
    "k = 3\n",
    "\n",
    "# Initialize the KNN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=k)\n",
    "\n",
    "# \"Training\" step: store X_train and y_train\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict labels for the test set\n",
    "predicted_labels = knn.predict(X_test)\n",
    "\n",
    "\n",
    "## Evaluating Accuracy\n",
    "\n",
    "We now measure how often the model predicts the correct label.\n",
    "\n",
    "**Accuracy** is defined as:\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\text{Number of correct predictions}}{\\text{Total number of samples}}\n",
    "$$\n",
    "Where:\n",
    "\n",
    "- **Number of correct predictions**: test samples where prediction = true label.\n",
    "- **Total number of samples**: all test samples.\n",
    "\n",
    "On a balanced dataset like MNIST, accuracy is a useful overall performance metric.\n",
    "\n",
    "##%%\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(y_test, predicted_labels)\n",
    "print(\"Test set accuracy:\", accuracy)\n",
    "\n",
    "##%% md\n",
    "## Confusion Matrix\n",
    "\n",
    "Accuracy gives a single number, but it does not show **which classes are confused** with each other.\n",
    "\n",
    "A **confusion matrix** gives a more detailed view:\n",
    "\n",
    "- It is a square matrix of size `number_of_classes √ó number_of_classes`.\n",
    "- **Rows** = Actual classes \n",
    "- **Columns** = Predicted classes \n",
    "- Entry (i, j) = number of samples of class *i* predicted as class *j*.\n",
    "\n",
    "We will also visualize it with a **heatmap**:\n",
    "\n",
    "- `annot=True` ‚Üí display counts inside cells.\n",
    "- `fmt=\"d\"` ‚Üí show integers.\n",
    "- Helps identify digits that the model often misclassifies.\n",
    "\n",
    "##%%\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(y_test, predicted_labels)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "##%%\n",
    "\n",
    "num_examples = 20 # show first 20 examples\n",
    "for i in range(num_examples):\n",
    "print(f\"Sample {i}: Actual = {y_test[i]}, Predicted = {predicted_labels[i]}\")\n",
    "##%% md\n",
    "This cell evaluates the K-Nearest Neighbors (KNN) classifier on the test dataset using class-specific performance metrics beyond simple accuracy. The classification_report function computes:\n",
    "\n",
    "1. Precision for each class:\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives + False Positives}}\n",
    "$$\n",
    "\n",
    "Measures the proportion of correctly predicted instances among all instances predicted as that class.\n",
    "\n",
    "High precision indicates a low false positive rate.\n",
    "\n",
    "***\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives + False Negatives}}\n",
    "$$\n",
    "\n",
    "Measures the proportion of correctly predicted instances among all actual instances of that class.\n",
    "\n",
    "High recall indicates that most actual instances of the class are correctly identified.\n",
    "***\n",
    "\n",
    "$$\n",
    "\\text{F1-Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision + Recall}}\n",
    "$$\n",
    "\n",
    "Harmonic mean of precision and recall, providing a single metric that balances both.\n",
    "***\n",
    "\n",
    "**Explanation of terms:**\n",
    "\n",
    "- **True Positives (TP):** \n",
    "Cases where the model correctly predicts the positive class. \n",
    "**Example:** Actual = 1, Predicted = 1 \n",
    "The model said ‚Äúpositive‚Äù and it was truly positive\n",
    "\n",
    "- **False Positives (FP):** \n",
    "Cases where the model predicts positive, but the actual class is negative. \n",
    "**Example:** Actual = 0, Predicted = 1 \n",
    "The model gave a ‚Äúpositive‚Äù prediction when it should be negative \n",
    "\n",
    "- **False Negatives (FN):** \n",
    "Cases where the model predicts negative, but the actual class is positive. \n",
    "**Example:** Actual = 1, Predicted = 0 \n",
    "The model missed a positive case\n",
    "- **Total number of samples:** Total number of instances in the test set.\n",
    "\n",
    "\n",
    "##%%\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(y_test, predicted_labels)\n",
    "print(report)\n",
    "\n",
    "##%% md\n",
    "**Digit 0**\n",
    "\n",
    "- Precision = 0.97 ‚Üí 97% of predicted 0‚Äôs are correct\n",
    "- Recall = 0.99 ‚Üí model finds 99% of true 0‚Äôs\n",
    "- F1 = 0.98 ‚Üí excellent\n",
    "- Support = 980 ‚Üí number of true 0‚Äôs in the test set\n",
    "\n",
    "\n",
    "GET TO WORK \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression classifier\n",
    "print(\"Training Logistic Regression classifier...\")\n",
    "lr_classifier = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    random_state=42,\n",
    "    multi_class='multinomial',  # For multi-class classification\n",
    "    solver='lbfgs'  # Good for small to medium datasets\n",
    ")\n",
    "\n",
    "lr_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_lr = lr_classifier.predict(X_test_scaled)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_lr = accuracy_score(y_test, y_pred_lr)\n",
    "print(f\"\\nLogistic Regression Accuracy: {accuracy_lr:.4f} ({accuracy_lr*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1 Logistic Regression - Confusion Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix for Logistic Regression\n",
    "cm_lr = confusion_matrix(y_test, y_pred_lr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=[f'Class {i}' for i in unique_emotions],\n",
    "            yticklabels=[f'Class {i}' for i in unique_emotions])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Logistic Regression - Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2 Logistic Regression - Classification Report\n",
    "in image data there is 3 CSV I WANT TO CREATE. Logistic regression & kmeans as classifiers on an image dataset (5 classes at maximum). IN JUPYTER NOTEBOOK \n",
    "\n",
    "HERE IS THE IMAGE PREPROCCESING TURTORIALS TA GAVE \n",
    "\n",
    "##%%\n",
    "#importing required Libraries\n",
    "import numpy as np\n",
    "import tensorflow\n",
    "import keras\n",
    "import os\n",
    "import glob\n",
    "from skimage import io\n",
    "import skimage\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "##%% md\n",
    "# Image Preprocessing in Python (Lecture Notebook)\n",
    "\n",
    "This notebook introduces fundamental image preprocessing techniques using **Python**, **OpenCV**, **scikit-image**, and **Keras**.\n",
    "\n",
    "We will cover:\n",
    "\n",
    "- Loading and visualizing images\n",
    "- RGB channels and grayscale conversion\n",
    "- Normalization\n",
    "- Data augmentation (flipping and rotation)\n",
    "- Contrast enhancement (Histogram Equalization & CLAHE)\n",
    "- Smoothing filters (mean, Gaussian, median)\n",
    "- Edge detection (Sobel, Laplacian)\n",
    "- Sharpening with custom kernels\n",
    "- Thresholding and basic image segmentation\n",
    "\n",
    "##%% md\n",
    "## 1. Loading and Visualizing the Original Image\n",
    "\n",
    "In this step we:\n",
    "\n",
    "1. Load an image from disk using `skimage.io.imread`.\n",
    "2. Visualize it using `matplotlib`.\n",
    "\n",
    "Why this step is important:\n",
    "\n",
    "- It lets us inspect the **raw image**.\n",
    "- We can observe:\n",
    "- Resolution (width √ó height)\n",
    "- Colors and lighting\n",
    "- Any visible noise or blur\n",
    "- This serves as a **baseline** before applying preprocessing.\n",
    "\n",
    "> Make sure `fruits.png` is in the same folder as your notebook, or provide the full path.\n",
    "\n",
    "##%%\n",
    "import os\n",
    "# accessing an image file from the dataset classes\n",
    "image = io.imread('fruits.png')\n",
    "\n",
    "# plotting the original image\n",
    "i, (im1) = plt.subplots(1)\n",
    "i.set_figwidth(15)\n",
    "im1.imshow(image)\n",
    "##%% md\n",
    "## 2. Visualizing the Original Image and Its RGB Color Channels\n",
    "\n",
    "Color images are stored as 3D arrays: **(height, width, channels)**.\n",
    "\n",
    "- Channel 0 ‚Üí Red\n",
    "- Channel 1 ‚Üí Green\n",
    "- Channel 2 ‚Üí Blue\n",
    "\n",
    "In this section we:\n",
    "\n",
    "- Show the original image\n",
    "- Show each channel separately (R, G, B)\n",
    "\n",
    "Why this helps:\n",
    "\n",
    "- Understand which colors dominate the image.\n",
    "- See how information is distributed across channels.\n",
    "- Useful before performing color-based processing (e.g. segmentation, enhancement).\n",
    "\n",
    "##%%\n",
    "# plotting the original image and the RGB channels\n",
    "\n",
    "i, (im1, im2, im3, im4) = plt.subplots(1, 4, sharey=True) # sharey -> All subplots will share the same Y-axis.\n",
    "i.set_figwidth(20)\n",
    "print(image.shape)\n",
    "im1.imshow(image) #Original image\n",
    "im2.imshow(image[:, : , 0]) #Red\n",
    "im3.imshow(image[:, : , 1]) #Green\n",
    "im4.imshow(image[:, : , 2]) #Blue\n",
    "i.suptitle('Original & RGB image channels')\n",
    "##%% md\n",
    "The RGB image is converted into a **grayscale** representation using `skimage.color.rgb2gray()`.\n",
    "A grayscale image reduces the three color channels (Red, Green, and Blue) into a single intensity channel by applying a weighted sum that reflects human visual perception. This simplification is commonly used in many preprocessing tasks, such as edge detection, thresholding, and filtering, where color information is not required.\n",
    "\n",
    "The resulting grayscale image is then displayed using matplotlib with the 'gray' colormap to ensure proper visualization of intensity variations.\n",
    "##%% md\n",
    "## 3. Converting RGB Image to Grayscale\n",
    "\n",
    "We convert the RGB image into a **single-channel grayscale** image using:\n",
    "\n",
    "```python\n",
    "skimage.color.rgb2gray(image) ---> it is automatically normalizes the grayscale image to the range [0, 1].\n",
    "\n",
    "##%%\n",
    "gray_image = skimage.color.rgb2gray(image)\n",
    "plt.imshow(gray_image, cmap = 'gray')\n",
    "##%% md\n",
    "## 4. Normalization (Min‚ÄìMax Scaling)\n",
    "\n",
    "We normalize the grayscale image to the range **[0, 1]** using:\n",
    "\n",
    "$$\n",
    "\\text{norm}(I) = \\frac{I - I_{\\min}}{I_{\\max} - I_{\\min}}\n",
    "$$\n",
    "\n",
    "Why normalize?\n",
    "\n",
    "- Standardizes pixel values.\n",
    "- Improves numerical stability for ML/DL models.\n",
    "- Helps networks train faster and more reliably.\n",
    "- Makes images from different sources more comparable.\n",
    "\n",
    "We will compute the normalized image and display it.\n",
    "\n",
    "##%%\n",
    "norm_image = (gray_image - np.min(gray_image)) / (np.max(gray_image) - np.min(gray_image))\n",
    "plt.imshow(norm_image)\n",
    "##%% md\n",
    "## 5. Installing `keras_preprocessing` (For Data Augmentation)\n",
    "\n",
    "To perform geometric transformations (data augmentation) in Keras, we use:\n",
    "\n",
    "- `ImageDataGenerator` from `keras_preprocessing.image`\n",
    "\n",
    "If the package is not already installed, we install it using `pip` inside the notebook.\n",
    "\n",
    "##%%\n",
    "#!pip install keras_preprocessing\n",
    "##%% md\n",
    "## 6. Preparing Image Batch for Data Augmentation\n",
    "\n",
    "`ImageDataGenerator` expects input as a batch of images with shape:\n",
    "\n",
    "- `(batch_size, height, width, channels)`\n",
    "\n",
    "We will:\n",
    "\n",
    "1. Convert our image to `float32`.\n",
    "2. Add a new axis to create a batch of size 1 using `np.expand_dims`.\n",
    "\n",
    "This will be used for flipping and rotation examples.\n",
    "\n",
    "##%%\n",
    "from numpy import expand_dims\n",
    "print(image.shape)\n",
    "# Ensure we use the RGB image for augmentation\n",
    "samples = expand_dims(image.astype('float32'), axis=0) # shape: (1, H, W, 3)\n",
    "\n",
    "\n",
    "##%%\n",
    "print(\"Batch shape:\", samples.shape)\n",
    "\n",
    "##%% md\n",
    "# Geometric Transformations\n",
    "In this step, we apply **geometric transformations** to the images using the `ImageDataGenerator` class from `Keras`. Geometric transformations are a form of data augmentation.\n",
    "\n",
    "We use `ImageDataGenerator` to apply:\n",
    "\n",
    "- `horizontal_flip=True` ‚Üí flip left‚Äìright\n",
    "- `vertical_flip=True` ‚Üí flip top‚Äìbottom\n",
    "\n",
    "Why do this?\n",
    "\n",
    "- Data augmentation: increases dataset size without collecting new images.\n",
    "- Makes models more robust to orientation changes.\n",
    "- Helps prevent overfitting.\n",
    "\n",
    "Explain:\n",
    "\n",
    "* Creating an ImageDataGenerator instance with the desired transformations.\n",
    "\n",
    "* Generating batches of images using the `.flow()` method, which produces transformed images on the fly.\n",
    "\n",
    "* Iterating through the generated images and converting them to unsigned integers (`uint8`) for proper visualization.\n",
    "\n",
    "* Plotting the transformed images side by side to observe the effects of horizontal and vertical flips.\n",
    "\n",
    "We will:\n",
    "\n",
    "1. Create an `ImageDataGenerator` with flipping options.\n",
    "2. Generate 3 augmented images.\n",
    "3. Display them side by side.\n",
    "##%%\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# ImageDataGenerator for flipping\n",
    "datagen = ImageDataGenerator(horizontal_flip=True, vertical_flip=True)\n",
    "\n",
    "# Create an iterator\n",
    "it = datagen.flow(samples, batch_size=1) #batch_size=1 ->Each time you call the iterator, it returns only 1 augmented image.\n",
    "\n",
    "# Plot some flipped images\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))\n",
    "\n",
    "for i in range(3):\n",
    "# Get a batch (1 image), take first image\n",
    "batch = next(it)[0].astype('uint8')\n",
    "axes[i].imshow(batch)\n",
    "axes[i].set_title(f\"Flip Sample {i+1}\")\n",
    "axes[i].axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"Horizontal & Vertical Flips\")\n",
    "plt.show()\n",
    "\n",
    "##%% md\n",
    "## 8. Geometric Transformations ‚Äì Rotation\n",
    "\n",
    "\n",
    "Apply `random rotation transformations` to the images as part of data augmentation. The `rotation_range` parameter specifies the maximum rotation angle (in degrees) for randomly rotating images. Here, `rotation_range=40` allows images to rotate within ¬±40 degrees.\n",
    "\n",
    "**Why use rotation?**\n",
    "- Simulates different orientations.\n",
    "- Helps models generalize better when objects are rotated in real-world data.\n",
    "##%%\n",
    "# ImageDataGenerator for rotation\n",
    "# Fills empty pixels with the value of the nearest pixel\n",
    "# The image will be randomly rotated between ‚Äì40¬∞ and +40¬∞.\n",
    "datagen = ImageDataGenerator(rotation_range=40, fill_mode='nearest') #\n",
    "\n",
    "# Create an iterator\n",
    "it = datagen.flow(samples, batch_size=1)\n",
    "\n",
    "# Plot some flipped images\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))\n",
    "\n",
    "for i in range(3):\n",
    "# Get a batch (1 image), take first image\n",
    "batch = next(it)[0].astype('uint8')\n",
    "axes[i].imshow(batch)\n",
    "axes[i].set_title(f\"Rotation Sample {i+1}\")\n",
    "axes[i].axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"Random Rotations (¬±40¬∞)\")\n",
    "plt.show()\n",
    "##%% md\n",
    "## 9. Histogram Equalization for Contrast Enhancement (ÿ≤ŸäÿßÿØÿ© ÿßŸÑÿ™ÿ®ÿßŸäŸÜ + ÿ™ÿ≠ÿ≥ŸäŸÜ ÿßŸÑŸàÿ∂Ÿàÿ≠)\n",
    "\n",
    "We now switch to **OpenCV** (`cv2`) for some operations.\n",
    "\n",
    "### Goal:\n",
    "- Improve image contrast using **histogram equalization**.\n",
    "\n",
    "Steps:\n",
    "1. Load the image in **grayscale**.\n",
    "2. Apply `cv2.equalizeHist`.\n",
    "3. Compare original vs equalized images.\n",
    "\n",
    "Histogram equalization:\n",
    "- Spreads pixel intensities (ÿ®ÿ™Ÿàÿ≤Ÿëÿπ ŸÇŸäŸÖ ÿßŸÑÿ•ÿ∂ÿßÿ°ÿ©) across the full range `[0, 255]`.\n",
    "- Makes dark regions brighter and bright regions clearer. (ÿ•ÿ∏Ÿáÿßÿ± ÿßŸÑÿ™ŸÅÿßÿµŸäŸÑ ÿßŸÑŸÖÿÆŸÅŸäÿ©)\n",
    "- Helpful when the image looks too dark or too washed out. (ŸÖŸÅŸäÿØ ŸÅŸä ÿßŸÑÿµŸàÿ± ÿ∞ÿßÿ™ ÿßŸÑÿ•ÿ∂ÿßÿ°ÿ© ÿßŸÑÿ≥Ÿäÿ¶ÿ©)\n",
    "\n",
    "##%%\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load grayscale image\n",
    "img = cv2.imread(\"fruits.png\", 0) # 0 ‚Üí grayscale\n",
    "\n",
    "# Histogram equalization\n",
    "equalized = cv2.equalizeHist(img)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Original Grayscale Image\")\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Equalized Image\")\n",
    "plt.imshow(equalized, cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "##%% md\n",
    "## 10. Visualizing Histograms Before and After Equalization\n",
    "\n",
    "To better understand the effect of **histogram equalization**, we plot the pixel intensity histograms of the original and equalized images.\n",
    "\n",
    "* `The original histogram` shows the distribution of pixel values in the grayscale image. In many cases, the values are concentrated in a narrow range, which can make the image appear dark or washed out.\n",
    "\n",
    "* `The equalized histogram` illustrates how the pixel intensities have been redistributed across the full range (0‚Äì255). This spreading of values increases the contrast and highlights previously hidden details.\n",
    "\n",
    "By comparing these histograms side by side, it becomes evident that histogram equalization effectively enhances image contrast while preserving the overall structure of the image. Visual inspection alongside histogram analysis provides a clear and quantitative understanding of the preprocessing step‚Äôs impact.\n",
    "##%%\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Original Histogram\")\n",
    "plt.hist(img.ravel(), 256, [0, 256])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Equalized Histogram\")\n",
    "plt.hist(equalized.ravel(), 256, [0, 256])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "##%% md\n",
    "## 11. Contrast Enhancement Using CLAHE\n",
    "\n",
    "**CLAHE** = Contrast Limited Adaptive Histogram Equalization.\n",
    "- used to improve the contrast of a color image\n",
    "\n",
    "Differences vs normal histogram equalization:\n",
    "\n",
    "- Works on **small tiles** instead of whole image. \n",
    "- Limits contrast amplification (avoids over-enhancing noise). ( Ÿäÿ≠ÿØ ŸÖŸÜ ÿ™ÿ∂ÿÆŸäŸÖ ÿßŸÑÿ™ÿ®ÿßŸäŸÜ (Ÿäÿ™ÿ¨ŸÜÿ® ÿßŸÑÿ™ÿ¥ŸàŸäÿ¥ ÿßŸÑŸÖŸÅÿ±ÿ∑).)\n",
    "- Very useful for images with **uneven lighting**. (ÿßŸÑÿ•ÿ∂ÿßÿ°ÿ© ÿ∫Ÿäÿ± ÿßŸÑŸÖÿ™ÿ≥ÿßŸàŸäÿ©)\n",
    "- A small amount of noise may appear due to processing each tile separately.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Convert BGR image to LAB color space. (LAB separates lightness (L) from color (A and B).)\n",
    "2. Apply CLAHE on the L (lightness) channel.\n",
    "3. Merge channels and convert back to BGR/RGB.\n",
    "4. Compare before and after.\n",
    "\n",
    "**LAB channels:**\n",
    "- L = Lightness (0‚Äì255)\n",
    "- A = Green‚ÄìRed scale\n",
    "- B = Blue‚ÄìYellow scale\n",
    "##%% md\n",
    "**clipLimit=3.0**\n",
    "- Prevents over-amplifying noise\n",
    "- Higher value ‚Üí stronger contrast\n",
    "\n",
    "**tileGridSize=(8, 8)**\n",
    "- Image is divided into 8√ó8 regions\n",
    "- Contrast enhancement is applied locally\n",
    "- Good for images with uneven lighting\n",
    "##%%\n",
    "# Load image (BGR)\n",
    "img_color = cv2.imread(\"fruits.png\")\n",
    "\n",
    "# Convert to LAB color space\n",
    "lab = cv2.cvtColor(img_color, cv2.COLOR_BGR2LAB)\n",
    "l, a, b = cv2.split(lab)\n",
    "\n",
    "# Apply CLAHE on L-channel (lightness)\n",
    "clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))\n",
    "cl = clahe.apply(l)\n",
    "\n",
    "# Merge channels and convert back to BGR\n",
    "lab_clahe = cv2.merge((cl, a, b))\n",
    "final = cv2.cvtColor(lab_clahe, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "# Show Results\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Original Image\")\n",
    "plt.imshow(cv2.cvtColor(img_color, cv2.COLOR_BGR2RGB))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"After CLAHE\")\n",
    "plt.imshow(cv2.cvtColor(final, cv2.COLOR_BGR2RGB))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "##%% md\n",
    "## 12. Low-Pass Filters (Smoothing / Blurring)\n",
    "\n",
    "In this step, we apply a **Mean Filter** to the grayscale image. The mean filter is a type of low-pass filter that **smooths** the image by reducing intensity variations between neighboring pixels. **It works by replacing each pixel value with the average of its surrounding pixels defined by a kernel** (in this case, a 3√ó3 window).\n",
    "\n",
    "\n",
    "We will apply:\n",
    "\n",
    "1. **Mean Filter** (Ÿäÿ£ÿÆÿ∞ ŸÖÿ™Ÿàÿ≥ÿ∑ ŸÇŸäŸÖ ÿßŸÑÿ®ŸÉÿ≥ŸÑÿßÿ™ ÿØÿßÿÆŸÑ ŸÜÿßŸÅÿ∞ÿ© (Kernel).)\n",
    "2. **Gaussian Blur**\n",
    "3. **Median Filter** \n",
    "\n",
    "Why smoothing?\n",
    "\n",
    "- Reduces noise.\n",
    "- Softens edges.\n",
    "- Often used before edge detection or segmentation.\n",
    "##%%\n",
    "# Reload grayscale image for filtering\n",
    "img = cv2.imread(\"fruits.png\", 0)\n",
    "\n",
    "# Mean (average) filter with 3x3 kernel\n",
    "mean = cv2.blur(img, (3, 3))\n",
    "\n",
    "plt.imshow(mean, cmap=\"gray\")\n",
    "plt.title(\"Mean Filter (3x3)\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "##%%\n",
    "# Gaussian blur with 5x5 kernel, sigma = 0 (auto)\n",
    "gaussian = cv2.GaussianBlur(img, (5, 5), 0)\n",
    "\n",
    "plt.imshow(gaussian, cmap=\"gray\")\n",
    "plt.title(\"Gaussian Blur (5x5)\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "##%%\n",
    "median = cv2.medianBlur(img, 5) # 5x5 neighborhood\n",
    "\n",
    "plt.imshow(median, cmap=\"gray\")\n",
    "plt.title(\"Median Filter (5x5)\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "##%% md\n",
    "## 13. High-Pass Filters (Edge Detection & Sharpening)\n",
    "\n",
    "Edge Detection Using **Sobel Filter**\n",
    "\n",
    "The Sobel operator computes the gradient of pixel intensities(ÿ™ÿØÿ±ÿ¨ ÿ¥ÿØÿ© ÿßŸÑÿ®ŸÉÿ≥ŸÑ) in both horizontal (X) and vertical (Y) directions, highlighting regions with significant intensity changes.\n",
    "\n",
    "* sobel(x) detects vertical edges by calculating horizontal intensity gradients.\n",
    "\n",
    "* sobel(y) detects horizontal edges by calculating vertical intensity gradients.\n",
    "\n",
    "Edge detection is a fundamental preprocessing step in computer vision, used to identify object boundaries, enhance features for segmentation, and extract structural information from images. Visualizing both X and Y gradients separately allows us to analyze the directionality and strength of edges in the image.\n",
    "##%% md\n",
    "**Understanding dx and dy**\n",
    "\n",
    "**- dx = 1, dy = 0 ‚Üí detect changes along X-axis**\n",
    "- Finds vertical edges\n",
    "- (Because vertical edges change in X direction)\n",
    "\n",
    "**- dx = 0, dy = 1 ‚Üí detect changes along Y-axis**\n",
    "- Finds horizontal edges\n",
    "- (Because horizontal edges change in Y direction)\n",
    "##%%\n",
    "sobelx = cv2.Sobel(img, cv2.CV_64F, 1, 0, ksize=3) # depth,1->SobelX, 0->SobelY\n",
    "sobely = cv2.Sobel(img, cv2.CV_64F, 0, 1, ksize=3) \n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(sobelx, cmap=\"gray\")\n",
    "plt.title(\"Sobel X (Vertical Edges)\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(sobely, cmap=\"gray\")\n",
    "plt.title(\"Sobel Y (Horizontal Edges)\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "##%% md\n",
    "### üîç Laplacian Filter (Simplified Explanation)\n",
    "\n",
    "The **Laplacian filter** is a high-pass filter used to detect edges in an image.\n",
    "\n",
    "- It responds to **sharp changes in intensity**.\n",
    "- Unlike Sobel (which finds horizontal or vertical edges separately), \n",
    "**Laplacian detects edges in all directions at once**.\n",
    "- This makes it useful for highlighting **fine details** and **object boundaries**.\n",
    "\n",
    "The resulting image shows bright edges and suppresses smooth, low-detail areas.\n",
    "\n",
    "##%%\n",
    "laplacian = cv2.Laplacian(img, cv2.CV_64F)\n",
    "\n",
    "plt.imshow(laplacian, cmap=\"gray\")\n",
    "plt.title(\"Laplacian (All Edges)\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "##%% md\n",
    "### Sharpening Filter (Custom Kernel)\n",
    "\n",
    "In this step, we apply a sharpening filter to enhance the details and edges of the grayscale image. The filter is implemented using a convolution kernel that emphasizes the central pixel relative to its neighbors:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "0 & -1 & 0 \\\\\n",
    "-1 & 5 & -1 \\\\\n",
    "0 & -1 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "- The center pixel is multiplied by 5.\n",
    "- Neighboring pixels are subtracted.\n",
    "- This increases contrast at edges ‚Üí image looks sharper.\n",
    "- This is a **balanced** sharpening kernel.\n",
    "\n",
    "This kernel increases the contrast between a pixel and its surrounding pixels, effectively highlighting edges and fine structures while maintaining the overall brightness of the image. Sharpening is a common preprocessing technique to improve the visibility of important features, which can be useful in tasks such as object recognition, segmentation, and feature extraction.\n",
    "##%%\n",
    "kernel = np.array([[0,-1,0],\n",
    "[-1,5,-1],\n",
    "[0,-1,0]])\n",
    "\n",
    "sharpened = cv2.filter2D(img, -1, kernel)\n",
    "\n",
    "plt.imshow(sharpened, cmap=\"gray\")\n",
    "plt.title(\"Sharpened Image\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "##%% md\n",
    "## 14. Segmentation with Thresholding\n",
    "\n",
    "### üåì Binary Thresholding\n",
    "\n",
    "In this step, we apply binary thresholding to the grayscale image. Thresholding is a fundamental technique in image segmentation.\n",
    "\n",
    "Binary thresholding separates the image into **two groups of pixels**: \n",
    "- **Foreground (white)** \n",
    "- **Background (black)** \n",
    "\n",
    "We use `cv2.threshold()` with a threshold value of **127**:\n",
    "\n",
    "- Pixels with intensity values **‚â• 127** ‚Üí become **255 (white)** \n",
    "- Pixels with intensity values **< 127** ‚Üí become **0 (black)** \n",
    "\n",
    "This creates a clean **black-and-white** image that highlights the main shapes and removes most background noise. \n",
    "Binary thresholding is commonly used before tasks like object detection, shape analysis, and feature extraction.\n",
    "\n",
    "##%%\n",
    "# Global binary thresholding\n",
    "_, thresh = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY) #255 maxval->The value assigned to pixels above the threshold\n",
    "\n",
    "plt.imshow(thresh, cmap=\"gray\")\n",
    "plt.title(\"Binary Threshold (T = 127)\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "##%% md\n",
    "### üåì Adaptive Thresholding (Simplified)\n",
    "\n",
    "Adaptive thresholding is used when the image has **uneven lighting**. \n",
    "Instead of using one global threshold, it calculates a **separate threshold for each small region** of the image.\n",
    "\n",
    "We use `cv2.adaptiveThreshold()` with these settings:\n",
    "\n",
    "- **ADAPTIVE_THRESH_MEAN_C** \n",
    "Threshold = (mean of local neighborhood) ‚àí C\n",
    "\n",
    "- **THRESH_BINARY** \n",
    "Output pixels become either **0 (black)** or **255 (white)**.\n",
    "\n",
    "- **blockSize = 11** \n",
    "Size of the small region used to compute the local mean.\n",
    "\n",
    "- **C = 2** \n",
    "A small constant that adjusts the threshold.\n",
    "\n",
    "This method produces a cleaner binary image in areas with different lighting, making segmentation more accurate.\n",
    "\n",
    "##%%\n",
    "adaptive = cv2.adaptiveThreshold(img, 255,\n",
    "cv2.ADAPTIVE_THRESH_MEAN_C,\n",
    "cv2.THRESH_BINARY,\n",
    "11, 2)\n",
    "\n",
    "plt.imshow(adaptive, cmap=\"gray\")\n",
    "plt.title(\"Adaptive Threshold\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "##%% md\n",
    "### üìà Otsu‚Äôs Thresholding (Automatic Binarization)\n",
    "\n",
    "Otsu‚Äôs method is a global thresholding technique that **automatically chooses the best threshold value** based on the image histogram. \n",
    "No need to set a threshold manually.\n",
    "\n",
    "Using `cv2.threshold()` with the `THRESH_OTSU` flag:\n",
    "\n",
    "- The optimal threshold is computed from the histogram.\n",
    "- Pixels **above** that value ‚Üí **255 (white)**\n",
    "- Pixels **below** ‚Üí **0 (black)**\n",
    "\n",
    "Otsu‚Äôs method works best when the image has a **bimodal histogram** (two clear intensity groups). It produces a clean binary image even when the contrast varies.\n",
    "\n",
    "##%%\n",
    "# Otsu's thresholding (automatic global threshold)\n",
    "_, otsu = cv2.threshold(\n",
    "img,\n",
    "0, # ignored when using OTSU\n",
    "255,\n",
    "cv2.THRESH_BINARY + cv2.THRESH_OTSU\n",
    ")\n",
    "\n",
    "plt.imshow(otsu, cmap=\"gray\")\n",
    "plt.title(\"Otsu Thresholding\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "##%% md\n",
    "## 15. Summary of Techniques Covered\n",
    "\n",
    "In this notebook, you learned how to:\n",
    "\n",
    "- Load and display images with `skimage` and `matplotlib`\n",
    "- Visualize RGB channels and convert to grayscale\n",
    "- Normalize pixel intensities to [0, 1]\n",
    "- Use `ImageDataGenerator` for:\n",
    "- Flipping\n",
    "- Rotation\n",
    "- Enhance contrast using:\n",
    "- Histogram Equalization\n",
    "- CLAHE (local contrast)\n",
    "- Apply smoothing filters:\n",
    "- Mean, Gaussian, Median\n",
    "- Detect edges using:\n",
    "- Sobel and Laplacian filters\n",
    "- Sharpen images using a custom convolution kernel\n",
    "- Perform basic segmentation with:\n",
    "- Global thresholding\n",
    "- Adaptive thresholding\n",
    "- Otsu's method\n",
    "\n",
    "These operations form a strong foundation for **image preprocessing** in computer vision pipelines before feeding images into machine learning or deep learning models.\n",
    "\n",
    "\n",
    "\n",
    "AND CLASSIFIER USING KNN HE GAVE TOO \n",
    "\n",
    "\n",
    "##%% md\n",
    "# K-Nearest Neighbors (KNN) for Image Classification\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook demonstrates the application of the **K-Nearest Neighbors (KNN)** algorithm to the task of **image classification**.\n",
    "\n",
    "KNN is a supervised learning method that classifies data points based on the labels of their nearest neighbors in the feature space. In the context of image classification, each image is represented as a vector of features, which may be raw pixel values or extracted features from a preprocessing pipeline.\n",
    "\n",
    "The primary objective of this notebook is to illustrate the process of:\n",
    "\n",
    "- Training a KNN classifier on a labeled image dataset.\n",
    "- Predicting the class labels of unseen test images.\n",
    "- Evaluating the model's performance using standard metrics.\n",
    "\n",
    "Throughout the notebook, we will:\n",
    "\n",
    "- Train a KNN classifier on the MNIST handwritten digits dataset.\n",
    "- Predict class labels for test images.\n",
    "- Evaluate performance using:\n",
    "- Accuracy\n",
    "- Confusion Matrix\n",
    "- Class-specific metrics (Precision, Recall, F1-score)\n",
    "\n",
    "##%%\n",
    "# Importing the dataset from Keras\n",
    "from keras.datasets import mnist\n",
    "\n",
    "# Load data: (x_train, y_train) for training, (x_test, y_test) for testing\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "##%% md\n",
    "## Loading the MNIST Dataset\n",
    "\n",
    "In this step, we load the **MNIST dataset** using `keras.datasets`.\n",
    "\n",
    "- `x_train`, `x_test`: contain the image data as NumPy arrays.\n",
    "- `y_train`, `y_test`: contain the corresponding labels (digits from 0 to 9).\n",
    "\n",
    "The dataset consists of:\n",
    "\n",
    "- 60,000 training images\n",
    "- 10,000 test images\n",
    "\n",
    "Each image is a grayscale `28 √ó 28` pixel image of a handwritten digit. \n",
    "- A grayscale image has one channel (not RGB).\n",
    "- Each pixel is a value from 0 ‚Üí 255:\n",
    "- 0 = black\n",
    "- 255 = white\n",
    "- values in between (1‚Äì254) = different shades of gray\n",
    "##%%\n",
    "# Checking the data types\n",
    "print(type(x_train))\n",
    "print(type(x_test))\n",
    "print(type(y_train))\n",
    "print(type(y_test))\n",
    "\n",
    "# Checking the shapes\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"x_test shape:\", x_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "##%%\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.gray() # Display images in black & white\n",
    "\n",
    "plt.figure(figsize=(10, 9)) # Adjust figure size\n",
    "\n",
    "# Display a grid of 3√ó3 images\n",
    "for i in range(9):\n",
    "plt.subplot(3, 3, i + 1)\n",
    "plt.imshow(x_train[i])\n",
    "\n",
    "plt.suptitle(\"Sample MNIST Training Images\")\n",
    "plt.show()\n",
    "\n",
    "##%%\n",
    "# Display the first 5 labels (true digit classes)\n",
    "for i in range(5):\n",
    "print(f\"Image {i} label:\", y_train[i])\n",
    "\n",
    "##%%\n",
    "# Checking the minimum and maximum values of x_train before normalization\n",
    "print(\"Before normalization:\")\n",
    "print(\"x_train min:\", x_train.min())\n",
    "print(\"x_train max:\", x_train.max())\n",
    "\n",
    "##%% md\n",
    "## Normalization for KNN\n",
    "\n",
    "We perform two preprocessing steps:\n",
    "\n",
    "1. **Data type conversion** \n",
    "- Convert from integer (`uint8`) to `float32` to support arithmetic operations and ML algorithms.\n",
    "\n",
    "2. **Normalization to [0, 1]** \n",
    "- Divide pixel values by 255.0.\n",
    "- Ensures that all features (pixels) contribute proportionally when computing distances.\n",
    "\n",
    "This is especially important for **KNN**, which relies on distance calculations in feature space. \n",
    "Without normalization, dimensions with larger numeric ranges could dominate the distance.\n",
    "\n",
    "##%%\n",
    "# Data Normalization\n",
    "\n",
    "# 1. Convert to float32\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# 2. Scale pixel values to [0, 1]\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "##%%\n",
    "# Checking the minimum and maximum values after normalization\n",
    "print(\"After normalization:\")\n",
    "print(\"x_train min:\", x_train.min())\n",
    "print(\"x_train max:\", x_train.max())\n",
    "\n",
    "##%% md\n",
    "## Flattening Images into Feature Vectors\n",
    "\n",
    "KNN (and many classical ML algorithms) expect input data as **2D arrays** of shape:\n",
    "\n",
    "- `(num_samples, num_features)`\n",
    "\n",
    "Originally:\n",
    "\n",
    "- `x_train` shape: `(num_samples, 28, 28)`\n",
    "\n",
    "After reshaping:\n",
    "\n",
    "- `X_train` shape: `(num_samples, 784)` where `784 = 28 √ó 28`.\n",
    "\n",
    "Each image becomes a **784-dimensional vector**, and KNN computes distances between these vectors to determine similarity.\n",
    "\n",
    "##%%\n",
    "# Reshaping input data\n",
    "# Each 28√ó28 image becomes a vector of length 784\n",
    "X_train = x_train.reshape(len(x_train), -1)\n",
    "X_test = x_test.reshape(len(x_test), -1)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "\n",
    "##%% md\n",
    "## K-Nearest Neighbors (KNN) as a Lazy Learner\n",
    "\n",
    "K-Nearest Neighbors (KNN) is considered a **lazy learning algorithm**.\n",
    "\n",
    "- Unlike **eager learning** methods (e.g., decision trees, neural networks) that **learn a global model** during training,\n",
    "- A **lazy learner** defers most computation until **prediction time**.\n",
    "- Lazy learners do NOT build a model during training.\n",
    "- They simply store the data and wait until a prediction is needed.\n",
    "\n",
    "### Key characteristics\n",
    "\n",
    "1. **Training Phase**\n",
    "- KNN does not learn explicit model parameters.\n",
    "- The `fit()` method simply **stores the training data and labels**.\n",
    "\n",
    "2. **Prediction Phase**\n",
    "- For each new input:\n",
    "- Computes the distance to all stored training points.\n",
    "- Finds the `k` nearest neighbors.\n",
    "- Predicts the class using **majority voting** among these neighbors.\n",
    "\n",
    "In this notebook, we will use `k = 3`, meaning each test image will be classified based on its 3 closest training images.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "When you choose k=3, the model:\n",
    "- Finds the 3 closest training samples to the test point\n",
    "- Looks at their labels\n",
    "- The majority vote becomes the prediction\n",
    "##%%\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Choose number of neighbors\n",
    "k = 3\n",
    "\n",
    "# Initialize the KNN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=k)\n",
    "\n",
    "# \"Training\" step: store X_train and y_train\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict labels for the test set\n",
    "predicted_labels = knn.predict(X_test)\n",
    "\n",
    "\n",
    "## Evaluating Accuracy\n",
    "\n",
    "We now measure how often the model predicts the correct label.\n",
    "\n",
    "**Accuracy** is defined as:\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\text{Number of correct predictions}}{\\text{Total number of samples}}\n",
    "$$\n",
    "Where:\n",
    "\n",
    "- **Number of correct predictions**: test samples where prediction = true label.\n",
    "- **Total number of samples**: all test samples.\n",
    "\n",
    "On a balanced dataset like MNIST, accuracy is a useful overall performance metric.\n",
    "\n",
    "##%%\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(y_test, predicted_labels)\n",
    "print(\"Test set accuracy:\", accuracy)\n",
    "\n",
    "##%% md\n",
    "## Confusion Matrix\n",
    "\n",
    "Accuracy gives a single number, but it does not show **which classes are confused** with each other.\n",
    "\n",
    "A **confusion matrix** gives a more detailed view:\n",
    "\n",
    "- It is a square matrix of size `number_of_classes √ó number_of_classes`.\n",
    "- **Rows** = Actual classes \n",
    "- **Columns** = Predicted classes \n",
    "- Entry (i, j) = number of samples of class *i* predicted as class *j*.\n",
    "\n",
    "We will also visualize it with a **heatmap**:\n",
    "\n",
    "- `annot=True` ‚Üí display counts inside cells.\n",
    "- `fmt=\"d\"` ‚Üí show integers.\n",
    "- Helps identify digits that the model often misclassifies.\n",
    "\n",
    "##%%\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(y_test, predicted_labels)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "##%%\n",
    "\n",
    "num_examples = 20 # show first 20 examples\n",
    "for i in range(num_examples):\n",
    "print(f\"Sample {i}: Actual = {y_test[i]}, Predicted = {predicted_labels[i]}\")\n",
    "##%% md\n",
    "This cell evaluates the K-Nearest Neighbors (KNN) classifier on the test dataset using class-specific performance metrics beyond simple accuracy. The classification_report function computes:\n",
    "\n",
    "1. Precision for each class:\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives + False Positives}}\n",
    "$$\n",
    "\n",
    "Measures the proportion of correctly predicted instances among all instances predicted as that class.\n",
    "\n",
    "High precision indicates a low false positive rate.\n",
    "\n",
    "***\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives + False Negatives}}\n",
    "$$\n",
    "\n",
    "Measures the proportion of correctly predicted instances among all actual instances of that class.\n",
    "\n",
    "High recall indicates that most actual instances of the class are correctly identified.\n",
    "***\n",
    "\n",
    "$$\n",
    "\\text{F1-Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision + Recall}}\n",
    "$$\n",
    "\n",
    "Harmonic mean of precision and recall, providing a single metric that balances both.\n",
    "***\n",
    "\n",
    "**Explanation of terms:**\n",
    "\n",
    "- **True Positives (TP):** \n",
    "Cases where the model correctly predicts the positive class. \n",
    "**Example:** Actual = 1, Predicted = 1 \n",
    "The model said ‚Äúpositive‚Äù and it was truly positive\n",
    "\n",
    "- **False Positives (FP):** \n",
    "Cases where the model predicts positive, but the actual class is negative. \n",
    "**Example:** Actual = 0, Predicted = 1 \n",
    "The model gave a ‚Äúpositive‚Äù prediction when it should be negative \n",
    "\n",
    "- **False Negatives (FN):** \n",
    "Cases where the model predicts negative, but the actual class is positive. \n",
    "**Example:** Actual = 1, Predicted = 0 \n",
    "The model missed a positive case\n",
    "- **Total number of samples:** Total number of instances in the test set.\n",
    "\n",
    "\n",
    "##%%\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(y_test, predicted_labels)\n",
    "print(report)\n",
    "\n",
    "##%% md\n",
    "**Digit 0**\n",
    "\n",
    "- Precision = 0.97 ‚Üí 97% of predicted 0‚Äôs are correct\n",
    "- Recall = 0.99 ‚Üí model finds 99% of true 0‚Äôs\n",
    "- F1 = 0.98 ‚Üí excellent\n",
    "- Support = 980 ‚Üí number of true 0‚Äôs in the test set\n",
    "\n",
    "\n",
    "GET TO WORK \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report for Logistic Regression\n",
    "print(\"Logistic Regression - Classification Report:\")\n",
    "print(\"=\" * 60)\n",
    "report_lr = classification_report(y_test, y_pred_lr, \n",
    "                                  target_names=[f'Class {i}' for i in unique_emotions])\n",
    "print(report_lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. K-Means Classifier\n",
    "\n",
    "K-Means is an unsupervised clustering algorithm. To use it for classification:\n",
    "1. Cluster the training data into k clusters (where k = number of classes)\n",
    "2. Map each cluster to a class label based on majority voting\n",
    "3. Predict test samples by assigning them to the nearest cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine number of clusters (should equal number of classes)\n",
    "n_clusters = len(unique_emotions)\n",
    "print(f\"Number of clusters: {n_clusters}\")\n",
    "\n",
    "# Train K-Means on training data\n",
    "print(\"\\nTraining K-Means classifier...\")\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10, max_iter=300)\n",
    "kmeans.fit(X_train_scaled)\n",
    "\n",
    "# Get cluster assignments for training data\n",
    "train_clusters = kmeans.predict(X_train_scaled)\n",
    "\n",
    "# Map clusters to class labels using majority voting\n",
    "cluster_to_class = {}\n",
    "for cluster_id in range(n_clusters):\n",
    "    # Find all training samples in this cluster\n",
    "    cluster_mask = (train_clusters == cluster_id)\n",
    "    cluster_labels = y_train[cluster_mask]\n",
    "    \n",
    "    # Find the most common class label in this cluster\n",
    "    if len(cluster_labels) > 0:\n",
    "        most_common_class = np.bincount(cluster_labels).argmax()\n",
    "        cluster_to_class[cluster_id] = most_common_class\n",
    "        print(f\"Cluster {cluster_id} -> Class {most_common_class} ({np.sum(cluster_labels == most_common_class)}/{len(cluster_labels)} samples)\")\n",
    "    else:\n",
    "        # If cluster is empty, assign to first class\n",
    "        cluster_to_class[cluster_id] = unique_emotions[0]\n",
    "\n",
    "print(f\"\\nCluster to class mapping: {cluster_to_class}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict test samples using K-Means\n",
    "test_clusters = kmeans.predict(X_test_scaled)\n",
    "\n",
    "# Map cluster assignments to class labels\n",
    "y_pred_kmeans = np.array([cluster_to_class[cluster_id] for cluster_id in test_clusters])\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_kmeans = accuracy_score(y_test, y_pred_kmeans)\n",
    "print(f\"\\nK-Means Accuracy: {accuracy_kmeans:.4f} ({accuracy_kmeans*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1 K-Means - Confusion Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix for K-Means\n",
    "cm_kmeans = confusion_matrix(y_test, y_pred_kmeans)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_kmeans, annot=True, fmt='d', cmap='Oranges',\n",
    "            xticklabels=[f'Class {i}' for i in unique_emotions],\n",
    "            yticklabels=[f'Class {i}' for i in unique_emotions])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('K-Means - Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2 K-Means - Classification Report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report for K-Means\n",
    "print(\"K-Means - Classification Report:\")\n",
    "print(\"=\" * 60)\n",
    "report_kmeans = classification_report(y_test, y_pred_kmeans,\n",
    "                                      target_names=[f'Class {i}' for i in unique_emotions])\n",
    "print(report_kmeans)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Comparison of Both Classifiers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare accuracies\n",
    "print(\"=\" * 60)\n",
    "print(\"CLASSIFIER COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nLogistic Regression Accuracy: {accuracy_lr:.4f} ({accuracy_lr*100:.2f}%)\")\n",
    "print(f\"K-Means Accuracy:            {accuracy_kmeans:.4f} ({accuracy_kmeans*100:.2f}%)\")\n",
    "print(f\"\\nDifference: {abs(accuracy_lr - accuracy_kmeans):.4f} ({abs(accuracy_lr - accuracy_kmeans)*100:.2f}%)\")\n",
    "\n",
    "# Visual comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Logistic Regression confusion matrix\n",
    "sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues', ax=ax1,\n",
    "            xticklabels=[f'C{i}' for i in unique_emotions],\n",
    "            yticklabels=[f'C{i}' for i in unique_emotions])\n",
    "ax1.set_xlabel('Predicted')\n",
    "ax1.set_ylabel('Actual')\n",
    "ax1.set_title(f'Logistic Regression (Accuracy: {accuracy_lr*100:.2f}%)')\n",
    "\n",
    "# K-Means confusion matrix\n",
    "sns.heatmap(cm_kmeans, annot=True, fmt='d', cmap='Oranges', ax=ax2,\n",
    "            xticklabels=[f'C{i}' for i in unique_emotions],\n",
    "            yticklabels=[f'C{i}' for i in unique_emotions])\n",
    "ax2.set_xlabel('Predicted')\n",
    "ax2.set_ylabel('Actual')\n",
    "ax2.set_title(f'K-Means (Accuracy: {accuracy_kmeans*100:.2f}%)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Visualizing Some Predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some test predictions\n",
    "n_samples_to_show = 12\n",
    "indices = np.random.choice(len(X_test), n_samples_to_show, replace=False)\n",
    "\n",
    "fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, i in enumerate(indices):\n",
    "    # Reshape flattened image back to 2D for display\n",
    "    img = X_test[i].reshape(height, width)\n",
    "    \n",
    "    axes[idx].imshow(img, cmap='gray')\n",
    "    \n",
    "    # Get predictions\n",
    "    true_label = y_test[i]\n",
    "    pred_lr = y_pred_lr[i]\n",
    "    pred_kmeans = y_pred_kmeans[i]\n",
    "    \n",
    "    # Determine if predictions are correct\n",
    "    lr_correct = \"‚úì\" if pred_lr == true_label else \"‚úó\"\n",
    "    kmeans_correct = \"‚úì\" if pred_kmeans == true_label else \"‚úó\"\n",
    "    \n",
    "    title = f\"True: {true_label}\\nLR: {pred_lr} {lr_correct} | K-Means: {pred_kmeans} {kmeans_correct}\"\n",
    "    axes[idx].set_title(title, fontsize=9)\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.suptitle('Sample Test Predictions', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Summary and Conclusions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nDataset:\")\n",
    "print(f\"  - Total samples: {len(X)}\")\n",
    "print(f\"  - Training samples: {len(X_train)}\")\n",
    "print(f\"  - Test samples: {len(X_test)}\")\n",
    "print(f\"  - Number of classes: {len(unique_emotions)}\")\n",
    "print(f\"  - Image size: {height}√ó{width} pixels\")\n",
    "print(f\"  - Features per image: {X.shape[1]}\")\n",
    "\n",
    "print(f\"\\nClassifier Performance:\")\n",
    "print(f\"  - Logistic Regression: {accuracy_lr*100:.2f}%\")\n",
    "print(f\"  - K-Means:            {accuracy_kmeans*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nObservations:\")\n",
    "if accuracy_lr > accuracy_kmeans:\n",
    "    print(f\"  - Logistic Regression performs better (supervised learning advantage)\")\n",
    "else:\n",
    "    print(f\"  - K-Means performs better (unusual, may indicate good cluster structure)\")\n",
    "    \n",
    "print(f\"  - K-Means is unsupervised and doesn't use label information during training\")\n",
    "print(f\"  - Logistic Regression uses label information and is better suited for classification\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
